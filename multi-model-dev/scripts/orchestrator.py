#!/usr/bin/env python3
"""
Multi-Model Development Orchestrator (v2)

Async parallel execution of multiple LLMs with:
- Model interface abstraction
- Timeout handling and error recovery
- Output parsing for each model format
- Execution modes (simple, medium, complex, architectural)

Usage:
    from orchestrator import Orchestrator, ExecutionMode
    
    async def main():
        orch = Orchestrator()
        result = await orch.run("Build a REST API endpoint")
        print(result.consensus_code)
    
    asyncio.run(main())
"""

import asyncio
import json
import re
import subprocess
import time
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Protocol, Tuple, Any
from pathlib import Path


# =============================================================================
# EXECUTION MODES
# =============================================================================

class ExecutionMode(Enum):
    """Task execution modes based on complexity."""
    SIMPLE = "simple"           # <50 LOC: 1 model, no validator
    MEDIUM = "medium"           # 50-500 LOC: 2 primary + 1 validator
    COMPLEX = "complex"         # >500 LOC: 2 primary + 2 validators
    ARCHITECTURAL = "arch"      # System-wide: all 4 models


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ModelOutput:
    """Output from a single model execution."""
    model: str
    code: str
    explanation: str
    execution_time: float
    success: bool
    error: Optional[str] = None
    raw_output: str = ""
    score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def code_blocks(self) -> List[str]:
        """Extract code blocks from output."""
        pattern = r'```(?:\w+)?\n(.*?)```'
        return re.findall(pattern, self.raw_output, re.DOTALL)
    
    @property
    def primary_code(self) -> str:
        """Get the primary code block or full code."""
        blocks = self.code_blocks
        return blocks[0].strip() if blocks else self.code


@dataclass
class OrchestratorResult:
    """Complete result from orchestration."""
    task: str
    mode: ExecutionMode
    category: str
    consensus_code: str
    explanation: str
    outputs: List[ModelOutput]
    validation: Optional[Dict] = None
    execution_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        return {
            "task": self.task,
            "mode": self.mode.value,
            "category": self.category,
            "consensus_code": self.consensus_code,
            "explanation": self.explanation,
            "outputs": [
                {
                    "model": o.model,
                    "success": o.success,
                    "score": o.score,
                    "execution_time": o.execution_time,
                    "code_length": len(o.code),
                    "error": o.error
                }
                for o in self.outputs
            ],
            "validation": self.validation,
            "execution_time": self.execution_time,
            "metadata": self.metadata
        }


# =============================================================================
# MODEL INTERFACE
# =============================================================================

class ModelRunner(Protocol):
    """Protocol for model runners."""
    
    @property
    def name(self) -> str:
        """Model name."""
        ...
    
    async def run(self, prompt: str, timeout: float = 60.0) -> ModelOutput:
        """Run the model with the given prompt."""
        ...


class BaseModelRunner(ABC):
    """Base class for model runners with common functionality."""
    
    def __init__(self, timeout: float = 60.0):
        self.default_timeout = timeout
    
    @property
    @abstractmethod
    def name(self) -> str:
        pass
    
    @abstractmethod
    async def _execute(self, prompt: str) -> Tuple[str, str]:
        """Execute the model. Returns (code, raw_output)."""
        pass
    
    async def run(self, prompt: str, timeout: Optional[float] = None) -> ModelOutput:
        """Run with timeout and error handling."""
        timeout = timeout or self.default_timeout
        start_time = time.time()
        
        try:
            code, raw_output = await asyncio.wait_for(
                self._execute(prompt),
                timeout=timeout
            )
            execution_time = time.time() - start_time
            
            return ModelOutput(
                model=self.name,
                code=code,
                explanation=f"Generated by {self.name}",
                execution_time=execution_time,
                success=True,
                raw_output=raw_output
            )
            
        except asyncio.TimeoutError:
            return ModelOutput(
                model=self.name,
                code="",
                explanation="",
                execution_time=timeout,
                success=False,
                error=f"Timeout after {timeout}s"
            )
        except Exception as e:
            return ModelOutput(
                model=self.name,
                code="",
                explanation="",
                execution_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _parse_code_blocks(self, text: str) -> str:
        """Extract code from markdown code blocks."""
        pattern = r'```(?:\w+)?\n(.*?)```'
        matches = re.findall(pattern, text, re.DOTALL)
        return matches[0].strip() if matches else text
    
    def _build_prompt(self, task: str) -> str:
        """Build a standardized prompt for code generation."""
        return f"""You are an expert software engineer. 

Task: {task}

Requirements:
1. Write clean, production-ready code
2. Include proper error handling
3. Add type hints where applicable
4. Include brief comments for complex logic
5. Return code in a markdown code block

Provide your solution:"""


# =============================================================================
# MODEL IMPLEMENTATIONS
# =============================================================================

class ClaudeCodeRunner(BaseModelRunner):
    """Runner for Claude Code CLI."""
    
    @property
    def name(self) -> str:
        return "claude-code"
    
    async def _execute(self, prompt: str) -> Tuple[str, str]:
        proc = await asyncio.create_subprocess_exec(
            "claude", "-p", self._build_prompt(prompt),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        
        if proc.returncode != 0:
            raise RuntimeError(f"claude-code failed: {stderr.decode()}")
        
        raw_output = stdout.decode()
        code = self._parse_code_blocks(raw_output)
        return code, raw_output


class CodexRunner(BaseModelRunner):
    """Runner for Codex CLI."""
    
    @property
    def name(self) -> str:
        return "codex"
    
    async def _execute(self, prompt: str) -> Tuple[str, str]:
        proc = await asyncio.create_subprocess_exec(
            "codex", "-q", self._build_prompt(prompt),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        
        if proc.returncode != 0:
            raise RuntimeError(f"codex failed: {stderr.decode()}")
        
        raw_output = stdout.decode()
        code = self._parse_code_blocks(raw_output)
        return code, raw_output


class GeminiRunner(BaseModelRunner):
    """Runner for Gemini CLI."""
    
    @property
    def name(self) -> str:
        return "gemini"
    
    async def _execute(self, prompt: str) -> Tuple[str, str]:
        proc = await asyncio.create_subprocess_exec(
            "gemini", "-p", self._build_prompt(prompt),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        
        if proc.returncode != 0:
            raise RuntimeError(f"gemini failed: {stderr.decode()}")
        
        raw_output = stdout.decode()
        code = self._parse_code_blocks(raw_output)
        return code, raw_output


class GrokRunner(BaseModelRunner):
    """Runner for Grok API."""
    
    def __init__(self, api_key: Optional[str] = None, timeout: float = 60.0):
        super().__init__(timeout)
        self.api_key = api_key or os.getenv("GROK_API_KEY")
    
    @property
    def name(self) -> str:
        return "grok"
    
    async def _execute(self, prompt: str) -> Tuple[str, str]:
        if not self.api_key:
            raise RuntimeError("GROK_API_KEY not set")
        
        try:
            from grok_client import GrokClient
            client = GrokClient(api_key=self.api_key)
            response = client.code_task(prompt)
            code = response.first_code_block or response.content
            return code, response.content
        except ImportError:
            # Fallback to direct API call
            import aiohttp
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.x.ai/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-3",
                        "messages": [{"role": "user", "content": self._build_prompt(prompt)}],
                        "temperature": 0.3,
                        "max_tokens": 4096
                    }
                ) as resp:
                    if resp.status != 200:
                        raise RuntimeError(f"Grok API error: {resp.status}")
                    data = await resp.json()
                    content = data["choices"][0]["message"]["content"]
                    code = self._parse_code_blocks(content)
                    return code, content


# =============================================================================
# TASK ANALYSIS
# =============================================================================

# Category keywords
CATEGORY_KEYWORDS = {
    "frontend": ["react", "component", "frontend", "ui", "button", "form", "dashboard", "vue", "css", "html"],
    "backend": ["api", "endpoint", "service", "backend", "middleware", "database", "server", "rest", "graphql"],
    "devops": ["docker", "kubernetes", "ci/cd", "pipeline", "terraform", "deploy", "ansible", "helm"],
    "scripts": ["script", "automation", "bash", "cli", "tool", "cron", "workflow"],
    "data": ["data", "pipeline", "etl", "analytics", "sql", "pandas", "spark"],
    "architecture": ["architecture", "design", "system", "scale", "pattern", "microservice"]
}

# Model routing by category
CATEGORY_ROUTING = {
    "frontend": (["gemini", "claude-code"], "codex"),
    "backend": (["claude-code", "codex"], "gemini"),
    "devops": (["codex", "grok"], "claude-code"),
    "scripts": (["codex", "gemini"], "claude-code"),
    "data": (["codex", "claude-code"], "gemini"),
    "architecture": (["grok", "claude-code"], "codex"),
    "generic": (["claude-code", "codex"], "gemini")
}


def analyze_task(task: str) -> Tuple[str, ExecutionMode]:
    """
    Analyze task to determine category and execution mode.
    Uses TaskRouter for intelligent routing when available.
    
    Returns: (category, execution_mode)
    """
    try:
        from router import TaskRouter, Complexity
        
        router = TaskRouter()
        decision = router.route(task)
        
        # Map complexity to execution mode
        complexity_to_mode = {
            Complexity.TRIVIAL: ExecutionMode.SIMPLE,
            Complexity.SIMPLE: ExecutionMode.SIMPLE,
            Complexity.MEDIUM: ExecutionMode.MEDIUM,
            Complexity.COMPLEX: ExecutionMode.COMPLEX,
            Complexity.LARGE: ExecutionMode.ARCHITECTURAL
        }
        
        mode = complexity_to_mode.get(decision.complexity.level, ExecutionMode.MEDIUM)
        return decision.category.value, mode
        
    except ImportError:
        # Fallback to basic analysis
        task_lower = task.lower()
        
        category = "generic"
        max_matches = 0
        for cat, keywords in CATEGORY_KEYWORDS.items():
            matches = sum(1 for kw in keywords if kw in task_lower)
            if matches > max_matches:
                max_matches = matches
                category = cat
        
        complexity_indicators = {
            "simple": ["simple", "basic", "quick", "small", "function", "helper"],
            "complex": ["complex", "full", "complete", "system", "integration", "multi"],
            "architecture": ["architecture", "design", "scale", "enterprise", "platform"]
        }
        
        mode = ExecutionMode.MEDIUM
        for mode_name, indicators in complexity_indicators.items():
            if any(ind in task_lower for ind in indicators):
                if mode_name == "simple":
                    mode = ExecutionMode.SIMPLE
                elif mode_name == "complex":
                    mode = ExecutionMode.COMPLEX
                elif mode_name == "architecture":
                    mode = ExecutionMode.ARCHITECTURAL
                break
        
        return category, mode


def get_routing(category: str, mode: ExecutionMode, task: str = "") -> Tuple[List[str], List[str]]:
    """
    Get model routing based on category and mode.
    Uses TaskRouter for intelligent routing when available.
    
    Returns: (primary_models, validators)
    """
    try:
        from router import TaskRouter
        
        if task:
            router = TaskRouter()
            decision = router.route(task)
            return decision.primary_models, decision.validators
    except ImportError:
        pass
    
    # Fallback routing
    primaries, default_validator = CATEGORY_ROUTING.get(category, CATEGORY_ROUTING["generic"])
    
    if mode == ExecutionMode.SIMPLE:
        return [primaries[0]], []
    elif mode == ExecutionMode.MEDIUM:
        return primaries[:2], [default_validator]
    elif mode == ExecutionMode.COMPLEX:
        all_models = ["claude-code", "codex", "gemini", "grok"]
        second_validator = next(
            (m for m in all_models if m not in primaries and m != default_validator),
            None
        )
        validators = [default_validator]
        if second_validator:
            validators.append(second_validator)
        return primaries[:2], validators
    else:  # ARCHITECTURAL
        return ["claude-code", "codex", "gemini", "grok"], []


# =============================================================================
# SCORING
# =============================================================================

def score_output(output: ModelOutput, task: str) -> float:
    """Score a model output (0-100)."""
    if not output.success:
        return 0.0
    
    score = 50.0  # Base score
    code = output.code
    
    # Code presence
    if code and len(code) > 50:
        score += 10
    
    # Structure indicators
    if "def " in code or "class " in code or "function " in code:
        score += 10
    if "import " in code or "from " in code or "require" in code:
        score += 5
    
    # Documentation
    if '"""' in code or "'''" in code or "/**" in code:
        score += 10
    if "#" in code or "//" in code:
        score += 5
    
    # Error handling
    if "try" in code or "catch" in code or "except" in code:
        score += 5
    
    # Type hints (Python/TypeScript)
    if ": " in code and "->" in code:
        score += 5
    if ": string" in code or ": number" in code or ": boolean" in code:
        score += 5
    
    return min(score, 100.0)


# =============================================================================
# MERGING
# =============================================================================

def merge_outputs(outputs: List[ModelOutput], task: str) -> Tuple[str, str]:
    """
    Merge multiple outputs into consensus code.
    Uses ConsensusMerger for AST-based merging when available.
    
    Returns: (consensus_code, explanation)
    """
    successful = [o for o in outputs if o.success and o.code]
    
    if not successful:
        return "", "No successful outputs to merge"
    
    if len(successful) == 1:
        return successful[0].code, f"Single output from {successful[0].model}"
    
    # Try AST-based merging
    try:
        from merger import ConsensusMerger
        
        merger = ConsensusMerger()
        outputs_dict = {o.model: o.raw_output or o.code for o in successful}
        result = merger.merge(outputs_dict, task=task)
        
        if result.merged_code:
            components_info = ", ".join(f"{k}â†{v}" for k, v in result.components_used.items())
            explanation = f"Merged (score: {result.total_score:.0f}) | {components_info}"
            return result.merged_code, explanation
    except ImportError:
        pass  # Fall back to simple merging
    except Exception as e:
        pass  # Fall back on any error
    
    # Fallback: Score and rank outputs, use best
    scored = [(o, o.score) for o in successful]
    scored.sort(key=lambda x: x[1], reverse=True)
    
    best = scored[0][0]
    
    explanation_parts = [
        f"Primary: {best.model} (score: {best.score:.0f})",
        f"Alternatives: {', '.join(o.model for o, _ in scored[1:])}"
    ]
    
    return best.code, " | ".join(explanation_parts)


# =============================================================================
# ORCHESTRATOR
# =============================================================================

class Orchestrator:
    """
    Multi-model orchestrator with async parallel execution.
    """
    
    def __init__(
        self,
        grok_api_key: Optional[str] = None,
        default_timeout: float = 60.0,
        max_retries: int = 1
    ):
        self.default_timeout = default_timeout
        self.max_retries = max_retries
        
        # Initialize model runners
        self.runners: Dict[str, BaseModelRunner] = {
            "claude-code": ClaudeCodeRunner(timeout=default_timeout),
            "codex": CodexRunner(timeout=default_timeout),
            "gemini": GeminiRunner(timeout=default_timeout),
            "grok": GrokRunner(api_key=grok_api_key, timeout=default_timeout),
        }
    
    async def run(
        self,
        task: str,
        mode: Optional[ExecutionMode] = None,
        models: Optional[List[str]] = None,
        timeout: Optional[float] = None,
        verbose: bool = False,
        on_progress: Optional[Any] = None  # ProgressCallback type
    ) -> OrchestratorResult:
        """
        Run the orchestration pipeline.
        
        Args:
            task: Task description
            mode: Execution mode (auto-detected if None)
            models: Specific models to use (auto-routed if None)
            timeout: Per-model timeout
            verbose: Print progress
            on_progress: Callback for real-time progress updates
            
        Returns:
            OrchestratorResult with consensus code and metadata
        """
        # Import progress helpers
        try:
            from progress import (
                emit_progress, task_received, task_analyzed, models_selected,
                model_starting, model_completed, model_failed,
                validation_starting, validation_completed,
                merge_starting, merge_completed, orchestration_completed
            )
            has_progress = True
        except ImportError:
            has_progress = False
        
        async def emit(progress_func, *args, **kwargs):
            """Helper to emit progress if available."""
            if has_progress and on_progress:
                progress = progress_func(*args, **kwargs)
                await emit_progress(on_progress, progress)
        
        start_time = time.time()
        timeout = timeout or self.default_timeout
        
        # Emit: task received
        await emit(task_received, task)
        
        # Analyze task
        category, auto_mode = analyze_task(task)
        mode = mode or auto_mode
        
        # Emit: task analyzed
        await emit(task_analyzed, category, auto_mode.value, mode.value)
        
        if verbose:
            print(f"ðŸ“‹ Task: {task[:60]}...")
            print(f"ðŸ“ Category: {category}")
            print(f"âš™ï¸  Mode: {mode.value}")
        
        # Get routing
        if models:
            primaries = models
            validators = []
        else:
            primaries, validators = get_routing(category, mode, task)
        
        all_models = primaries + validators
        
        # Emit: models selected
        await emit(models_selected, primaries, validators)
        
        if verbose:
            print(f"ðŸŽ¯ Models: {', '.join(primaries)}")
            if validators:
                print(f"âœ… Validators: {', '.join(validators)}")
            print()
        
        # Run models in parallel (with progress)
        outputs = await self._run_parallel(all_models, task, timeout, verbose, on_progress)
        
        # Score outputs
        for output in outputs:
            output.score = score_output(output, task)
        
        # Run validation if we have validators
        validation = None
        if validators and len([o for o in outputs if o.success]) >= 2:
            await emit(validation_starting, validators[0])
            validation = await self._run_validation(task, outputs, validators[0], verbose)
            if validation:
                await emit(
                    validation_completed,
                    validation.get("confidence", 0),
                    validation.get("needs_human_review", False)
                )
        
        # Merge outputs
        successful_count = len([o for o in outputs if o.success])
        await emit(merge_starting, successful_count)
        
        consensus_code, explanation = merge_outputs(outputs, task)
        
        # Calculate final score
        final_score = sum(o.score for o in outputs if o.success) / max(successful_count, 1)
        await emit(merge_completed, final_score, len(consensus_code))
        
        execution_time = time.time() - start_time
        
        # Emit: complete
        await emit(orchestration_completed, execution_time, final_score)
        
        if verbose:
            print(f"\nðŸŽ¯ Consensus: {len(consensus_code)} chars")
            print(f"â±ï¸  Total time: {execution_time:.1f}s")
        
        return OrchestratorResult(
            task=task,
            mode=mode,
            category=category,
            consensus_code=consensus_code,
            explanation=explanation,
            outputs=outputs,
            validation=validation,
            execution_time=execution_time,
            metadata={
                "primaries": primaries,
                "validators": validators,
                "timestamp": datetime.now().isoformat()
            }
        )
    
    async def _run_parallel(
        self,
        models: List[str],
        task: str,
        timeout: float,
        verbose: bool,
        on_progress: Optional[Any] = None
    ) -> List[ModelOutput]:
        """Run multiple models in parallel with progress updates."""
        
        # Import progress helpers if available
        try:
            from progress import emit_progress, model_starting, model_completed, model_failed
            has_progress = True
        except ImportError:
            has_progress = False
        
        async def emit(progress_func, *args, **kwargs):
            if has_progress and on_progress:
                progress = progress_func(*args, **kwargs)
                await emit_progress(on_progress, progress)
        
        async def run_with_retry(model: str) -> ModelOutput:
            runner = self.runners.get(model)
            if not runner:
                await emit(model_failed, model, f"Unknown model: {model}")
                return ModelOutput(
                    model=model, code="", explanation="",
                    execution_time=0, success=False,
                    error=f"Unknown model: {model}"
                )
            
            # Emit: model starting
            await emit(model_starting, model)
            
            for attempt in range(self.max_retries + 1):
                output = await runner.run(task, timeout)
                if output.success:
                    # Emit: model completed
                    await emit(model_completed, model, score_output(output, task), 
                              output.execution_time, len(output.code))
                    if verbose:
                        print(f"âœ… {model}: {len(output.code)} chars ({output.execution_time:.1f}s)")
                    return output
                
                if attempt < self.max_retries:
                    if verbose:
                        print(f"âš ï¸  {model}: retrying ({output.error})")
                    await asyncio.sleep(1)
            
            # Emit: model failed
            await emit(model_failed, model, output.error or "Unknown error")
            if verbose:
                print(f"âŒ {model}: {output.error}")
            return output
        
        # Run all models concurrently
        tasks = [run_with_retry(model) for model in models]
        outputs = await asyncio.gather(*tasks)
        
        return list(outputs)
    
    async def _run_validation(
        self,
        task: str,
        outputs: List[ModelOutput],
        validator_model: str,
        verbose: bool
    ) -> Optional[Dict]:
        """Run cross-model validation."""
        try:
            from validator import Validator
            
            if verbose:
                print(f"\nðŸ” Validating with {validator_model}...")
            
            outputs_dict = {o.model: o.code for o in outputs if o.success}
            
            def model_runner(model: str, prompt: str) -> str:
                # Sync wrapper for validation
                import asyncio
                runner = self.runners.get(model)
                if runner:
                    loop = asyncio.new_event_loop()
                    try:
                        output = loop.run_until_complete(runner.run(prompt))
                        return output.raw_output or output.code
                    finally:
                        loop.close()
                raise RuntimeError(f"No runner for {model}")
            
            validator = Validator(model_runner=model_runner)
            result = validator.validate(task, outputs_dict, validator_model)
            
            if verbose:
                status = "âš ï¸ Review needed" if result.needs_human_review else "âœ… Passed"
                print(f"{status} (confidence: {result.confidence:.0%})")
            
            return result.to_dict()
            
        except ImportError:
            if verbose:
                print("âš ï¸  Validator not available")
            return None
        except Exception as e:
            if verbose:
                print(f"âš ï¸  Validation error: {e}")
            return None
    
    def run_sync(self, task: str, **kwargs) -> OrchestratorResult:
        """Synchronous wrapper for run()."""
        return asyncio.run(self.run(task, **kwargs))
    
    async def run_with_updates(
        self,
        task: str,
        progress_handler: Optional[Any] = None,
        **kwargs
    ) -> OrchestratorResult:
        """
        Run with real-time progress updates.
        
        If no progress_handler provided, uses ConsoleProgress.
        For Telegram/OpenClaw integration, pass an OpenClawProgress or TelegramProgress.
        
        Args:
            task: Task description
            progress_handler: ProgressHandler instance (or uses console)
            **kwargs: Additional args passed to run()
            
        Returns:
            OrchestratorResult
            
        Example:
            from progress import OpenClawProgress, TelegramProgress
            
            # Console output (default)
            result = await orch.run_with_updates(task)
            
            # Telegram integration
            telegram = TelegramProgress(bot_token, chat_id)
            result = await orch.run_with_updates(task, telegram)
        """
        if progress_handler is None:
            try:
                from progress import ConsoleProgress
                progress_handler = ConsoleProgress(verbose=True)
            except ImportError:
                pass
        
        callback = progress_handler.emit if progress_handler else None
        return await self.run(task, on_progress=callback, **kwargs)
    
    def run_with_updates_sync(self, task: str, **kwargs) -> OrchestratorResult:
        """Synchronous wrapper for run_with_updates()."""
        return asyncio.run(self.run_with_updates(task, **kwargs))


# =============================================================================
# CLI
# =============================================================================

async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Multi-Model Orchestrator")
    parser.add_argument("task", nargs="?", help="Task to execute")
    parser.add_argument("--mode", "-m", choices=["simple", "medium", "complex", "arch"])
    parser.add_argument("--models", nargs="+", help="Specific models to use")
    parser.add_argument("--timeout", "-t", type=float, default=60.0)
    parser.add_argument("--verbose", "-v", action="store_true")
    parser.add_argument("--json", "-j", action="store_true", help="Output JSON")
    
    args = parser.parse_args()
    
    if not args.task:
        parser.print_help()
        return
    
    mode = ExecutionMode(args.mode) if args.mode else None
    
    orch = Orchestrator()
    result = await orch.run(
        args.task,
        mode=mode,
        models=args.models,
        timeout=args.timeout,
        verbose=args.verbose
    )
    
    if args.json:
        print(json.dumps(result.to_dict(), indent=2))
    else:
        print("\n" + "=" * 60)
        print("CONSENSUS CODE")
        print("=" * 60)
        print(result.consensus_code)
        print("=" * 60)
        print(f"\nExplanation: {result.explanation}")
        print(f"Total time: {result.execution_time:.1f}s")


if __name__ == "__main__":
    asyncio.run(main())
